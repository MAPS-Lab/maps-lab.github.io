<!DOCTYPE html>
<html lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <style>
            body {
                visibility: hidden;
            }
        </style>
        <!-- font style, ask pro for content, change link in nav,  -->
        <meta name="viewport" content="width=device-width, initial-scale=1" />
        <meta name="description" content="MAPS Lab University of Edinburgh" />
        <title>MAPS-Lab</title>
        <link rel="shortcut icon" href="uploads/castle.png">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.1/css/bulma.min.css" />
        <script src="https://www.w3schools.com/lib/w3.js"></script>
        <script src="uploads/maps.js"></script>
    </head>

    <body style="visibility: visible;">
        <!--Start Navigation-->
        <nav class="navbar has-shadow" role="navigation" aria-label="main navigation">
            <div class="container">
                <div class="navbar-brand" style=" display: contents;">
                    <a class="navbar-item" href="https://maps-lab.github.io/">
                        <img height="100px" style="max-height: 5.0rem"src="uploads/maps-lab-icon.png" />
                    </a>
                    <div class="navbar-burger burger" data-target="navbarExampleTransparentExample">
                        <span></span>
                        <span></span>
                        <span></span>
                    </div>
                </div>
                <div class="navbar-menu" id="navbarExampleTransparentExample">
                    <div class="navbar-end">
                        <a class="navbar-item" href="https://maps-lab.github.io/">
                            <div  style = "font-size: larger;">Home</div>
                        </a>
                        <a class="navbar-item" href="people.html">
                            <div  style = "font-size: larger;">People</div>
                        </a>
                        <a class="navbar-item active" href="research.html">
                            <div  style = "font-size: larger;">Research</div>
                        </a>
                        <a class="navbar-item" href="publication.html">
                            <div style = "font-size: larger;">Publications</div>
                        </a>
                        <a class="navbar-item" href="contact.html">
                            <div style = "font-size: larger;">Contact</div>
                        </a>
                    </div>
                </div>
            </div>
        </nav>
        <!--End   Navigation-->
        <!--Start Main body-->
        <section class="section">
            <div class="container">
            <p class="title is-4" style="margin-top: -20px; font-size: 2rem;font-family: Arial;">Reseach Projects</p>
            <p style="margin-bottom: 2em; font-size: 1.2rem;font-family: Arial;">Ongoing and completed research projects by MAPS Lab</p>
            <!--start project-->
            <div class="columns">
                <div class="column is-12">
                    <div class="box">
                        <div style="text-align: center;">
                            <p class="title is-8" style="margin-bottom: 0.6rem; font-size: 1.7rem;font-family: Cambria;">Robust Spatial Perception for Autonomous Vehicles in the Wild</p>
                        </div>
                        <div>
                            <figure class="image">
                              <img src="uploads/projects/radar_perception.png" alt="Research" />
                            </figure>
                        </div>
                        <div class="block has-text-justified">
                        <p style="margin-bottom: 0.5rem;margin-top: 1rem;font-family: Cambria; font-size: 1.3rem; "> 
                        Autonomous driving, aka. mobile autonomy, promises to radically change the cityscape and save many human lives. A pillar component for achieving mobile autonomy is <b>spatial perception - 
                        the ability for vehicles to understand the ambient environment and localize themselves on the road</b>. Thanks to the recent advances in computer vision and solid-state technology, great 
                        improvement in spatial perception has been witnessed in controlled environments. Nevertheless, the perception robustness of many systems is still far off the safety requirement of autonomous 
                        driving when it comes to the wild condition, such as bad weather, poor illumination, various dynamic objects or even malicious attacks. In the MAPS lab, we study the robust spatial perception problems in full-stack, 
                        ranging from the single-modality based methods to multi-sensory fusion, and to the perception uncertainty quantification. More recently, a traction to my lab is unfolding the potential of <b>4D automotive radar in localization and scene understanding</b>. 
                        As an emerging sensor, automotive radars are reputable for their sensing robustness against bad weather and adverse illumination. However, due to their significantly lower data quality, it remains largely unknown how one can transform the 
                        radar sensing robustness to vehicle perception effectiveness - a question my team keen to answer.
                        </p>
                        <p style="margin-bottom: 0.5rem;margin-top: 0.5rem;font-family: Cambria; font-size: 1.3rem; "> 
                        <b>Research Output</b>:
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="https://www.research.ed.ac.uk/en/publications/metawave-attacking-mmwave-sensing-with-meta-material-enhanced-tag">[NDSS’2023],</a>
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="https://toytiny.github.io/publication/22-raflow-ral/index.html">[RA-L/IROS’2022],</a>
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="https://arxiv.org/abs/2203.01851">[IROS’2022a],</a>
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="https://www.pure.ed.ac.uk/ws/portalfiles/portal/289946438/Pedestrian_Liveness_LI_DOA18072022_AFV.pdf">[SECON’2022],</a>
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="https://intranet.csc.liv.ac.uk/~ramdrop/autoplace.html">[ICRA’2022a],</a>
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="https://qqqgpe.github.io/2022-02-11/DC-Loc">[ICRA’2022b],</a>
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="https://arxiv.org/abs/1912.13077">[TNNLS’2022],</a>
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="https://arxiv.org/pdf/1908.03918.pdf">[TNNLS’2021],</a>
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="https://arxiv.org/abs/1909.03557">[AAAI’2020],</a>
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="https://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Selective_Sensor_Fusion_for_Neural_Visual-Inertial_Odometry_CVPR_2019_paper.html">[CVPR’2019]</a>
                        </div>
                        </p>
                    </div>

                </div>

            </div>
            <div class="columns">
                <div class="column is-12">
                    <div class="box">
                        <div style="text-align: center;">
                            <p class="title is-8" style="margin-bottom: 0.6rem; font-size: 1.7rem;font-family: Cambria;">Robust and Rapid Sense Augmentation Support for First Responders</p>
                        </div>
                        <div>
                            <figure class="image">
                              <img src="uploads/projects/fire_fighter.png" alt="Research" />
                            </figure>
                        </div>
                        <div class="block has-text-justified">
                        <p style="margin-bottom: 0.5rem;margin-top: 1rem;font-family: Cambria; font-size: 1.3rem; "> 
                        From the equator to the Arctic, fire disasters are going to happen more often as a result of anthropogenic climate change. This consequently results in more frequent duty calls of firefighters. 
                        However, at the present, firefighting is still regarded as one of the most strenuous and dangerous jobs in the world. A fire incident is often accompanied by a variety of airborne obscurants (e.g., smoke and dust) 
                        and poor illumination, making firefighters difficult to navigate themselves and understand the fire ground. We aim to design robust yet real-time <b>localization, mapping and scene understanding</b>. services 
                        that can be directly integrated into the <b>augmented reality wearables and firefighting robots</b>. 
                        These support systems will, in turn, enhance firefighters’ operational capacity and safety in 
                        visually-degraded conditions and on resource-constrained platforms.
                        </p>
                        <p style="margin-bottom: 0.5rem;margin-top: 0.5rem;font-family: Cambria; font-size: 1.3rem; "> 
                        <b>Research Output</b>: 
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="https://arxiv.org/abs/2104.07196">[TR-O’2022],</a>
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="https://arxiv.org/abs/2206.01589">[IROS’2022b],</a>
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="https://arxiv.org/ftp/arxiv/papers/2112/2112.05665.pdf">[CPS-ER’2022],</a>
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="https://ieeexplore.ieee.org/document/9561738">[ICRA’2021],</a>
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="https://arxiv.org/abs/2006.02266">[SenSys’2020],</a>
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="https://arxiv.org/abs/1911.00398">[MobiSys’2020],</a>
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="https://ieeexplore.ieee.org/document/8968430">[RA-L’2020]</a>
                        </p>
                        <p style="margin-bottom: 0.5rem;margin-top: 0.5rem;font-family: Cambria; font-size: 1.3rem; "> 
                        <b>Media Exposure</b>: 
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="https://www.bbc.co.uk/news/av/uk-scotland-63075749">BBC News,</a>
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="https://www.bbc.co.uk/sounds/play/m001cg5x">BBC Good Morning Scotland,</a>
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="http://www.pressdata.co.uk/viewbroadcast.asp?a_id=27804277">STV,</a>
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="https://planetradio.co.uk/borders/local/news/firefighters-smart-helmets-heriot-watt/">Planet Radio,</a>
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="http://www.pressdata.co.uk/viewbroadcast.asp?a_id=27806734">Sky News,</a>
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="https://podcasts.apple.com/gb/podcast/ai-smart-helmets-give-firefighters-superhero-ability/id1516299890?i=1000580906865">Evening Standard Tech & Science Daily,</a>
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="https://www.scottishdailyexpress.co.uk/news/scottish-news/firefighters-could-soon-smart-helmets-28099105">Scottish Daily Express,</a>
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="https://www.scottishfield.co.uk/living/firefighters-get-hi-tech-help-from-robotarium/">Scotish Field,</a>
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="https://www.scottishdailyexpress.co.uk/news/scottish-news/firefighters-could-soon-smart-helmets-28099105">Scottish Daily Mail,</a>
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="https://www.independent.co.uk/news/uk/experts-scotland-edinburgh-innovation-university-of-edinburgh-b2176943.html">The independent,</a>
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="https://news.italy-24.com/trends/115673/Helmets-with-artificial-intelligence-to-help-firefighters.html">Italy 24 News,</a>
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="https://www.irishnews.com/magazine/technology/2022/09/28/news/firefighters_could_soon_have_smart_helmets_to_help_locate_blaze_victims-2842709/">Irish News,</a>
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="https://eandt.theiet.org/content/articles/2022/09/smart-helmets-could-help-firefighters-locate-blaze-victims/">Engineering & Technology,</a>
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="https://www.digit.fyi/scots-firefighters-national-robotarium-smart-helmet/">Digit News</a>
                        </p>

                        </div>
                    </div>

                </div>

            </div>
            <div class="columns">
                <div class="column is-12">
                    <div class="box">
                        <div style="text-align: center;">
                            <p class="title is-8" style="margin-bottom: 0.6rem; font-size: 1.7rem;font-family: Cambria;">Privacy-aware Low-Cost Human Sensing</p>
                        </div>
                        <div>
                            <figure class="image">
                              <img src="uploads/projects/human_sensing.png" alt="Research" />
                            </figure>
                        </div>
                        <div class="block has-text-justified">
                        <p style="margin-bottom: 0.5rem;margin-top: 1rem;font-family: Cambria; font-size: 1.3rem; "> 
                        Despite a plethora of methods being proposed in the last decades, today’s human sensing 
                        (e.g., who, where, and what activities) systems are mostly dominant by RGB camera-based 
                        approaches that suffer from the adverse lighting conditions or privacy concerns in domestic 
                        environments. On the other side of the problem, unlike designing methods for cameras, 
                        it is much more challenging for us to find design heuristics for <b>low-resolution sensors, 
                        e.g., mmWave, WiFi, UWB, BLE and inertial measurement units</b> - as humans do not use these modalities 
                        to perceive the world, nor relate our everyday experience in improving the algorithm for these sensors. 
                        By exploiting the recent advances in machine learning, we advocate <b>AI-empowered methods</b> to model the 
                        data from these sensors and push their limits to the envelope. A related question also of our interest 
                        is - beyond the coarse-grained localization, is it possible to achieve fine-grained pose estimation 
                        (e.g., gesture and hand movement etc.) with these low-cost and low-resolution sensors?
                        </p>
                        <p style="margin-bottom: 0.5rem;margin-top: 0.5rem;font-family: Cambria; font-size: 1.3rem; "> 
                        <b>Research Output</b>: 
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="https://arxiv.org/abs/2111.03976">[IoT-J’2023],</a>
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="https://arxiv.org/pdf/2207.07896.pdf">[UbiComp’2022],</a>
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="https://arxiv.org/pdf/2208.14326.pdf">[IoT-J’2022],</a>
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="https://arxiv.org/abs/2103.01055">[ICCV’2021],</a>
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="https://ieeexplore.ieee.org/document/8937008">[TMC’2021],</a>
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="https://ieeexplore.ieee.org/document/9547669">[TNNLS’2021],</a>
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="https://ieeexplore.ieee.org/document/9197437/">[ICRA’2020],</a>
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="https://ieeexplore.ieee.org/document/8402111">[TMC’2019],</a>
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="https://doi.org/10.1609/aaai.v33i01.33018009">[AAAI’2019],</a>
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="https://doi.org/10.1109/DCOSS.2019.00028">[DCOSS’2019],</a>
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="https://arxiv.org/abs/1802.02209">[AAAI’2018],</a>
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="http://www.cs.ox.ac.uk/files/10769/%5BMobiCom2018%5Demr_slam.pdf">[MobiCom’2018],</a>
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="https://ieeexplore.ieee.org/document/7293674/"> [TWC’2016]</a>
                        </p>
                        </div>
                    </div>

                </div>

            </div>
            <div class="columns">
                <div class="column is-12">
                    <div class="box">
                        <div style="text-align: center;">
                            <p class="title is-8" style="margin-bottom: 0.6rem; font-size: 1.7rem;font-family: Cambria;">Robust Identity Inference across Digital and Physical Worlds (completed)</p>
                        </div>
                        <div>
                            <figure class="image">
                              <img src="uploads/projects/IoT.png" alt="Research" />
                            </figure>
                        </div>
                        <div class="block has-text-justified">
                        <p style="margin-bottom: 0.5rem;margin-top: 1rem;font-family: Cambria; font-size: 1.3rem; "> 
                        Key to realizing the vision of human-centred computing is the ability for machines to recognize people, 
                        so that spaces and devices can become truly personalized. However, the unpredictability of real-world environments 
                        impacts robust recognition, limiting usability. In real conditions, human identification systems have to handle 
                        issues such as out-of-set subjects and domain deviations, where conventional supervised learning approaches for 
                        training and inference are poorly suited. With the rapid development of Internet of Things (IoT), we advocate a 
                        new labelling method that exploits signals of opportunity hidden in heterogeneous IoT data. The key insight is 
                        that <b>one sensor modality can leverage the signals measured by other co-located sensor modalities to improve its 
                        own labelling performance</b>. If identity associations between heterogeneous sensor data can be discovered, it is 
                        possible to automatically label data, leading to more robust human recognition, without manual labelling or enrolment. 
                        On the other side of the coin, we also study the privacy implication for such cross-modal identity association.
                        </p>
                        <p style="margin-bottom: 0.5rem;margin-top: 0.5rem;font-family: Cambria; font-size: 1.3rem; "> 
                        <b>Research Output</b>: 
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="https://arxiv.org/abs/2001.08211">[WWW’2020],</a>
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="https://arxiv.org/abs/1908.09002">[WWW’2019],</a>
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="https://ieeexplore.ieee.org/document/8755294">[IoT-J’2019],</a>
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="https://arxiv.org/abs/1912.04836">[UbiComp’2018],</a>
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="https://dl.acm.org/doi/10.1145/3267242.3267252">[ISWC’2018],</a>
                        <a style="color:rgb(101,167,12)" class="link-sytle" href="https://dl.acm.org/doi/10.1145/3055031.3055073">[IPSN’2017]</a>
                        </p>
                        </div>
                    </div>
                </div>

            </div>
            
            <!--end project-->
            </div>
        </section>

    </body>
</html>